{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special\n",
    "\n",
    "# Neural network class definition\n",
    "class neuralNetwork:\n",
    "    \n",
    "    # initialise the neural network\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes,\n",
    "                learningrate):\n",
    "        # set number of nodes in each input, hidden, output layers\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        # learning rate\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        # link weight matrices, wih and who\n",
    "        # weights inside the arrays are w_i_j, where link is from node i to node j in the next layer\n",
    "        self.wih = np.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes)) - 0.5        \n",
    "        self.who = np.random.normal(0.0, pow(self.hnodes, -0.5), (self.onodes, self.hnodes)) - 0.5\n",
    "        \n",
    "        # activiation function is the sigmoid function\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    # train the neural network\n",
    "    def train():\n",
    "        pass\n",
    "    \n",
    "    # query the neural network\n",
    "    def query(self, inputs_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        return final_outputs\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add that to out class definition of a nn, and try creating a small nn object with 3 nodes in each layer, and a learning rate of 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of input, hidden and output nodes\n",
    "input_nodes = 3\n",
    "hidden_nodes = 3\n",
    "output_nodes = 3\n",
    "\n",
    "# learning rate is 0.3\n",
    "learning_rate = 0.3\n",
    "\n",
    "#create instance of NN\n",
    "n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Weights - the Heart of Network*\n",
    "The most important part fo the network is the link weights. We saw earlier that the weights can be concisely expressed as a matrix. So we can create: $W_{input\\_hidden}$ of size hidden_nodes * input_nodes.\n",
    "\n",
    "$W_{hidden\\_output}$ of size output_nodes * hidden_nodes.\n",
    "\n",
    "__remember the convention earlier why the first matrix $hidden$ \\* $input$ rather than $input$ \\* $hidden$__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "init_weight = np.random.rand(3,3)-0.5 # to make initial random value [-0.5, 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use for notes, Do Not RUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code, with comments included, creates the two link weight matrices using the self.__inodes__, self.__hnodes__ and self.__onodes__ to set the right size for both of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link weight matrices, wih and who\n",
    "# weights inside the array are w_i_j, where link is from node i to node j in the next layer\n",
    "self.wih = np.random.rand(self.hnodes, self.inodes) - 0.5\n",
    "self.who = np.random.rand(self.onodes, self.hnodes) - 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or more sophisticated weigths, we can use normal distribution weight. Mean is 0 and sd is $\\frac{1}{\\sqrt{\\text{# of incoming links}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.wih = np.random.normal(0.0, pow(self.inodes, -0.5), self.hnodes, self.inodes) - 0.5\n",
    "self.who = np.random.normal(0.0, pow(self.hnodes, -0.5), self.onodes, self.hnodes) - 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work on query() function\n",
    "\n",
    "$X_{hidden} = W_{input\\_hidden}\\cdot I$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_inputs = np.dot(self.wih, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$O_{hidden} = sigmoid(X_{hidden})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy.special for the sigmoid function expit()\n",
    "import scipy.special"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define activation function only once inside the neural network object when it is first initialised. After that we can refer to it several times, such as in the query() fucntion. This arrangement means we only need to change this definition once, and not have to locate and change the code anywhere an activiation function is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activiation function is the sigmoid function\n",
    "self.activation_function = lambda x: scipy.special.expit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to task, we want to apply the activation function to the combined and moderated signals into the hidden nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the signals emerging from hidden layer\n",
    "hidden_outputs = self.activation_function(hidden_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, the signal emerging from the hidden layer nodes are in the matrix called __hidden_outputs__\n",
    "\n",
    "Then, from hidden to final output layer signal and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate signals into final output layer\n",
    "final_inputs = np.dot(self.who, hidden_outputs\n",
    "# calculate the signals emerging from final output layer\n",
    "final_outputs = self.activation_function(final_intputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go back to main codes and check the codes so far\n",
    "test codes so far, query() and class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of input, hidden and output nodes\n",
    "input_nodes = 3\n",
    "hidden_nodes = 3\n",
    "output_nodes = 3\n",
    "\n",
    "# learning rate is 0.3\n",
    "learning_rate = 0.3\n",
    "\n",
    "#create instance of NN\n",
    "n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n.query([1.0, 0.5, -1.5]) \n",
    "# work, everything is good so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Network\n",
    "Do Not Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the neural network\n",
    "def train(self, inputs_list, target_list):\n",
    "    # convert inputs list to 2d array\n",
    "    inputs = np.array(inputs_list, ndmin=2).T\n",
    "    targets = np.array(targets_list, ndmin=2).T\n",
    "       \n",
    "    # calculate signals into hidden layer\n",
    "    hidden_inputs = np.dot(self.wih, inputs)\n",
    "    # calculate the signals emerging from hidden layer\n",
    "    hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "    # calculate signals into final output layer\n",
    "    final_inputs = np.dot(self.who, hidden_outputs)\n",
    "    # calculate the signals emerging from final output layer\n",
    "    final_outputs = self.activation_function(final_inputs)\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to calculate the error. (targets - actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_errors = target - final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we calculate the back-propagated errors for the hidden layer nodes. Remeber how we split the errors according to the coneected weights, and recombine them for each hidden layer node\n",
    "\n",
    "$\\textbf{errors}_{hidden} = \\textbf{weights}_{hidden\\_output}^T \\cdot \\textbf{errors}_{output}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer error is the output_errors, split by weights, recombined at hidden nodes\n",
    "hidden_errors = np.dot(self.who.T, output_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to refine the weights at each layer. For the weigths between the hidden and final layers, we use the __output_errors__. For the weights between the input and hidden layers, we use these __hidden_errors__ we just calculate.\n",
    "\n",
    "We previously worked out the expression for updating the weight for the link between a node __j__ and a node __k__ in the next layer in matrix form.\n",
    "$$\\Delta w_{jk} = \\alpha * E_k * sigmoid(O_k) * (1- sigmoid(O_k)) \\cdot O_j^T$$\n",
    "The $\\alpha$ is learning rate, and the sigmoid is the squashing activation funciton. That last bit, the matrix of outputs from the previous layer, is transposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the weights for the links between the hidden and output layers\n",
    "self.who += self.lr*np.dot((outputs_errors*final_outputs*(1.0-final_outputs)),np.transpose(hidden_outputs))\n",
    "\n",
    "# update the weights for the links between the input and hidden layers\n",
    "self.wih += self.lr*np.dot((hidden_errors*hidden_outputs*(1.0-hidden_outputs)), np.transpose(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete the Neural Network Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network class definition\n",
    "class neuralNetwork:\n",
    "    \n",
    "    # initialise the neural network\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes,\n",
    "                learningrate):\n",
    "        # set number of nodes in each input, hidden, output layers\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        # learning rate\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        # link weight matrices, wih and who\n",
    "        # weights inside the arrays are w_i_j, where link is from node i to node j in the next layer\n",
    "        self.wih = np.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))        \n",
    "        self.who = np.random.normal(0.0, pow(self.hnodes, -0.5), (self.onodes, self.hnodes))\n",
    "        \n",
    "        # activiation function is the sigmoid function\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    # train the neural network\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        targets = np.array(targets_list, ndmin=2).T\n",
    "\n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        output_errors = targets - final_outputs\n",
    "        # hidden layer error is the output_errors, split by weights, recombined at hidden nodes\n",
    "        hidden_errors = np.dot(self.who.T, output_errors)\n",
    "        \n",
    "        # update the weights for the links between the hidden and output layers\n",
    "        self.who += self.lr*np.dot((output_errors*final_outputs*(1.0-final_outputs)),np.transpose(hidden_outputs))\n",
    "\n",
    "        # update the weights for the links between the input and hidden layers\n",
    "        self.wih += self.lr*np.dot((hidden_errors*hidden_outputs*(1.0-hidden_outputs)), np.transpose(inputs))\n",
    "        \n",
    "        pass        \n",
    "    \n",
    "    # query the neural network\n",
    "    def query(self, inputs_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        return final_outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
