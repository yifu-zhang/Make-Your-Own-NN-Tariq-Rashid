{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special\n",
    "\n",
    "# Neural network class definition\n",
    "class neuralNetwork:\n",
    "    \n",
    "    # initialise the neural network\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes,\n",
    "                learningrate):\n",
    "        # set number of nodes in each input, hidden, output layers\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        # learning rate\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        # link weight matrices, wih and who\n",
    "        # weights inside the arrays are w_i_j, where link is from node i to node j in the next layer\n",
    "        self.wih = np.random.normal(0.0, pow(self.inodes, -0.5), self.hnodes, self.inodes) - 0.5        \n",
    "        self.who = np.random.normal(0.0, pow(self.hnodes, -0.5), self.onodes, self.hnodes) - 0.5\n",
    "        \n",
    "        # activiation function is the sigmoid function\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    # train the neural network\n",
    "    def train():\n",
    "        pass\n",
    "    \n",
    "    # query the neural network\n",
    "    def query(self, inputs_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        return final_outputs\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add that to out class definition of a nn, and try creating a small nn object with 3 nodes in each layer, and a learning rate of 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of input, hidden and output nodes\n",
    "input_nodes = 3\n",
    "hidden_nodes = 3\n",
    "output_nodes = 3\n",
    "\n",
    "# learning rate is 0.3\n",
    "learning_rate = 0.3\n",
    "\n",
    "#create instance of NN\n",
    "n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Weights - the Heart of Network*\n",
    "The most important part fo the network is the link weights. We saw earlier that the weights can be concisely expressed as a matrix. So we can create: $W_{input\\_hidden}$ of size hidden_nodes * input_nodes.\\\n",
    "$W_{hidden\\_output}$ of size output_nodes * hidden_nodes.\\\n",
    "__remember the convention earlier why the first matrix $hidden$ * $input$ rather than $input$ * $hidden$__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "init_weight = np.random.rand(3,3)-0.5 # to make initial random value [-0.5, 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use for notes, Do Not RUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code, with comments included, creates the two link weight matrices using the self.__inodes__, self.__hnodes__ and self.__onodes__ to set the right size for both of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link weight matrices, wih and who\n",
    "# weights inside the array are w_i_j, where link is from node i to node j in the next layer\n",
    "self.wih = np.random.rand(self.hnodes, self.inodes) - 0.5\n",
    "self.who = np.random.rand(self.onodes, self.hnodes) - 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or more sophisticated weigths, we can use normal distribution weight. Mean is 0 and sd is $\\frac{1}{\\sqrt{\\text{# of incoming links}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.wih = np.random.normal(0.0, pow(self.inodes, -0.5), self.hnodes, self.inodes) - 0.5\n",
    "self.who = np.random.normal(0.0, pow(self.hnodes, -0.5), self.onodes, self.hnodes) - 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work on query() function\\\n",
    "$X_{hidden} = W_{input\\_hidden}\\cdot I$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_inputs = np.dot(self.wih, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$O_{hidden} = sigmoid(X_{hidden})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy.special for the sigmoid function expit()\n",
    "import scipy.special"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define activation function only once inside the neural network object when it is first initialised. After that we can refer to it several times, such as in the query() fucntion. This arrangement means we only need to change this definition once, and not have to locate and change the code anywhere an activiation function is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activiation function is the sigmoid function\n",
    "self.activation_function = lambda x: scipy.special.expit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to task, we want to apply the activation function to the combined and moderated signals into the hidden nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the signals emerging from hidden layer\n",
    "hidden_outputs = self.activation_function(hidden_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, the signal emerging from the hidden layer nodes are in the matrix called __hidden_outputs__\\\n",
    "Then, from hidden to final output layer signal and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate signals into final output layer\n",
    "final_inputs = np.dot(self.who, hidden_outputs\n",
    "# calculate the signals emerging from final output layer\n",
    "final_outputs = self.activation_function(final_intputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go back to main codes and check the codes so far\\\n",
    "P138"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
